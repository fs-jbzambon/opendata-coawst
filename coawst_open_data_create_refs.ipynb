{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a3a5c6e-97c3-44c0-9224-3756b6774479",
   "metadata": {},
   "source": [
    "# Create reference files for the COAWST forecast archive on AWS Open Data\n",
    "We use [kerchunk](https://fsspec.github.io/kerchunk/) to create individual reference files for each weekly NetCDF file, \n",
    "then create the combined JSON that allows access to the entire collection as a single dataset in Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-mayor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from kerchunk.combine import MultiZarrToZarr, auto_dask, JustLoad\n",
    "\n",
    "from dask.distributed import Client\n",
    "import dask.bag as db\n",
    "import ujson\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e8365-aa8a-4d06-8661-129855593fda",
   "metadata": {},
   "source": [
    "We can read from AWS Open Data using `anon=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-therapist",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs_read = fsspec.filesystem('s3', anon=True, skip_instance_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b949a69-7304-425b-876a-74d5f4cd0d92",
   "metadata": {},
   "source": [
    "We can't *write* to AWS Open Data without credentials, which we will specify through environment variables.  Because we are going to use environment variables instead of referencing an AWS profile, we don't specify `profile=` here in fs_write, but use `anon=False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-serial",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs_write = fsspec.filesystem('s3', anon=False, skip_instance_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-lightning",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flist = fs_read.glob('s3://usgs-coawst/useast-archive/*.nc')\n",
    "json_dir = 's3://usgs-coawst/useast-archive/json2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-abortion",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flist = [f's3://{f}' for f in flist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-logan",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(flist))\n",
    "print(flist[0])\n",
    "print(flist[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc01098-ca89-498f-9e80-c2b523525811",
   "metadata": {},
   "source": [
    "#### Create references for each NetCDF file in parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = dict(mode='rb', anon=True, skip_instance_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(u):\n",
    "    with fs_read.open(u, **so) as infile:\n",
    "        fname = Path(u).stem\n",
    "        h5chunks = SingleHdf5ToZarr(infile, u, inline_threshold=300)\n",
    "        outf = f'{json_dir}/{fname}.json'\n",
    "        with fs_write.open(outf, 'wb') as f:\n",
    "            f.write(ujson.dumps(h5chunks.translate()).encode());\n",
    "    return outf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fcf4f-048b-45d2-85af-7beb9d748893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "import nebari_tools as nbt\n",
    "\n",
    "aws_profile = 'coawst_open_data'\n",
    "aws_region = 'us-west-2'\n",
    "endpoint_url = f's3.{aws_region}.amazonaws.com'\n",
    "nbt.set_credentials(profile=aws_profile, region=aws_region, endpoint_url=endpoint_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a7cd1-7c6f-4ea4-9279-9d6f2ac50297",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_max = 30\n",
    "client,cluster = nbt.start_dask_cluster(profile=aws_profile, worker_max=worker_max, \n",
    "                                      region=aws_region, use_existing_cluster=True,\n",
    "                                      adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                      worker_profile='Small Worker', \n",
    "                                      propagate_env=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bag = db.from_sequence(flist, npartitions=worker_max).map(gen_json)\n",
    "bag.compute(retries=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636a01dd-334b-46fa-a7ec-52b2c3d567e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = fs_read.glob(f'{json_dir}/*.json')\n",
    "json_list = [f's3://{j}' for j in json_list]\n",
    "print(len(json_list))\n",
    "print(json_list[0])\n",
    "print(json_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def consolidate(out_):\n",
    "    for k, v in out_.items():\n",
    "        if isinstance(v, bytes):\n",
    "            try:\n",
    "              # easiest way to test if data is ascii\n",
    "                out_[k] = v.decode('ascii')\n",
    "            except UnicodeDecodeError:\n",
    "                out_[k] = (b\"base64:\" + base64.b64encode(v)).decode()\n",
    "        else:\n",
    "                out_[k] = v\n",
    "    return out_\n",
    "\n",
    "import zarr\n",
    "\n",
    "def modify_attrs(out):\n",
    "    out_= zarr.open(out)\n",
    "    out_.ocean_time.attrs['standard_name'] = 'time'\n",
    "    return out\n",
    "\n",
    "def preprocess(out):\n",
    "    out = modify_attrs(out)\n",
    "    out = consolidate(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-tucson",
   "metadata": {},
   "source": [
    "#### Create combined references json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "mzz = MultiZarrToZarr(json_list,   \n",
    "    concat_dims = ['ocean_time'],\n",
    "    coo_map={\"ocean_time\": \"cf:ocean_time\"},\n",
    "    identical_dims=['lat_psi','lat_rho','lat_u','lat_v',\n",
    "                    'lon_psi','lon_rho','lon_u','lon_v'],\n",
    "                     preprocess=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d = mzz.translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fs5 = fsspec.filesystem(\"reference\", fo=d, skip_instance_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = fs5.get_mapper(\"\")\n",
    "ds = xr.open_dataset(m, engine=\"zarr\", \n",
    "                     backend_kwargs={'consolidated':False}, chunks={}, \n",
    "                     drop_variables=['dstart'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_json = 'combined2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a80ca6-07ad-4a6d-90ed-3018886c60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with fs.open(combined_json, 'wb') as f:\n",
    "    f.write(ujson.dumps(d).encode());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f97c60-2773-4cef-8a79-9e711252486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_json_aws = f's3://usgs-coawst/useast-archive/{combined_json}'\n",
    "combined_json_aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3512f-88d7-4753-a178-e7b8e0a2dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_write.upload(combined_json, combined_json_aws)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-pangeo2",
   "language": "python",
   "name": "conda-env-global-global-pangeo2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
